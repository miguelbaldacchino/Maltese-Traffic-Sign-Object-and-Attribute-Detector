{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9b3de8",
   "metadata": {},
   "source": [
    "# Comparing Results between all Detectors\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook will go over different methods of comparing and assessing the perforamnces of each object detection model trained and used. These methods of evaluating and compaaring include accuracy metrics, inference speed, per-class performance, and qualitative comparison on samples. For complete fairness, each model was trained on the same version of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706f31da",
   "metadata": {},
   "source": [
    "### Models Compared:\n",
    "\n",
    "- `YOLOv8`:  1-stage detector mostly used for real-time solutions.\n",
    "- `YOLO AGAIN??? IDK WHO HAS THIS`:\n",
    "- `Faster R-CNN`:  2-Stage detector which focuses on optimising detection accuracy.\n",
    "- `SSD`:  1-stage detector and lightwieght to optimise speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb486774",
   "metadata": {},
   "source": [
    "#### Import necessary libraries and Quantitative Summary\n",
    "\n",
    "The script below summarises quantitative results for each model.Mean Average Precision (mAP) is reported at IoU thresholds of 0.5 and 0.5:0.95.\n",
    "Inference speed is measured in frames per second (FPS) on GPU hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdaf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "results_data = {\n",
    "    'Model': [                              #REPLACE\n",
    "        'YOLOv8 (This Work)',               #REPLACE\n",
    "        'Faster R-CNN (Member 2)',          #REPLACE\n",
    "        'SSD (Member 3)'\n",
    "    ],\n",
    "    'mAP@0.5': [0.92, 0.89, 0.85],          # Placeholder\n",
    "    'mAP@0.5:0.95': [0.75, 0.78, 0.65],     # Placeholder\n",
    "    'Inference_Speed_FPS': [45, 12, 55]     # Placeholder\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb74074",
   "metadata": {},
   "source": [
    "## 2. Comparing with mAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = df.melt(\n",
    "    id_vars=\"Model\",\n",
    "    value_vars=[\"mAP@0.5\", \"mAP@0.5:0.95\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_melted, x=\"Model\", y=\"Score\", hue=\"Metric\")\n",
    "plt.title(\"Model Accuracy Comparison (mAP)\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"mAP Score\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5195e33",
   "metadata": {},
   "source": [
    "## 3. Acuracy and Speed\n",
    "\n",
    "These 2 metrics usually need to be traded off depending on the project spwecificities and requirements, as each project will have a different balance between speed and accuracy. This section will visualise inference speed and detection accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"Inference_Speed_FPS\",\n",
    "    y=\"mAP@0.5:0.95\",\n",
    "    s=200\n",
    ")\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    plt.text(\n",
    "        df.Inference_Speed_FPS[i] + 1,\n",
    "        df['mAP@0.5:0.95'][i],\n",
    "        df.Model[i]\n",
    "    )\n",
    "\n",
    "plt.title(\"Inference Speed vs Accuracy\")\n",
    "plt.xlabel(\"Inference Speed (FPS) → Faster\")\n",
    "plt.ylabel(\"Accuracy (mAP@0.5:0.95) → Better\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f30796",
   "metadata": {},
   "source": [
    "## 4. Comparing models per Class\n",
    "\n",
    "Comparing models per class average precision will help us indetify any specifically worse or better performing model on an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with real per-class AP values\n",
    "per_class_data = pd.DataFrame({\n",
    "    'Class': ['Stop', 'No Entry', 'Pedestrian Crossing'],\n",
    "    'YOLOv8': [0.91, 0.88, 0.85],\n",
    "    'Faster R-CNN': [0.93, 0.90, 0.80],\n",
    "    'SSD': [0.85, 0.83, 0.75]\n",
    "})\n",
    "\n",
    "per_class_melted = per_class_data.melt(\n",
    "    id_vars=\"Class\",\n",
    "    var_name=\"Model\",\n",
    "    value_name=\"AP\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    data=per_class_melted,\n",
    "    x=\"Class\",\n",
    "    y=\"AP\",\n",
    "    hue=\"Model\"\n",
    ")\n",
    "plt.title(\"Per-Class Average Precision Comparison\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d16b0",
   "metadata": {},
   "source": [
    "### Training Behavoiur with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e806ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0fd8ed",
   "metadata": {},
   "source": [
    "### Training Metrics using Ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6740d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m results_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/detect/sign_detector/results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# adjust if needed\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m yolo_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(results_path)\n\u001b[0;32m      3\u001b[0m yolo_results\u001b[38;5;241m.\u001b[39mtail()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "results_path = \"runs/detect/sign_detector/results.csv\"  # adjust if needed\n",
    "yolo_results = pd.read_csv(results_path)\n",
    "yolo_results.tail()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(yolo_results['epoch'], yolo_results['metrics/mAP50(B)'], label='mAP@0.5')\n",
    "plt.plot(yolo_results['epoch'], yolo_results['metrics/mAP50-95(B)'], label='mAP@0.5:0.95')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Validation mAP Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f6dd1",
   "metadata": {},
   "source": [
    "## 5. Qualititave Comparison\n",
    "\n",
    "\n",
    "Apart from conducting a quantitative analysis, we can also produce a qualitiative comparison between the models by using test images, for example images with partial occlusion, or poor lighting on traffic signs.\n",
    "\n",
    "Visualisations of model predictions identify:\n",
    "\n",
    "- `Missed detection`\n",
    "- `False positives`\n",
    "- `Localisation errors on bounding boxes`\n",
    "\n",
    "These demonstrate where the model purely fails and demonstrates the weaknesses in the built dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24cc702",
   "metadata": {},
   "source": [
    "## 6. Discussion and Conclusion\n",
    "\n",
    "\n",
    "DO NOT FORHGET THIS!!!!!!!!!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARI3129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
